{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "from utils import count_params\n",
    "import preprocessing as preprocess\n",
    "import torch\n",
    "import numpy as np\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils import data\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "if torch.cuda.is_available():  \n",
    "    DEVICE = \"cuda:0\" \n",
    "else:  \n",
    "    DEVICE = \"cpu\"\n",
    "print(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter(comment = '_resnet20')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                     std=[0.229, 0.224, 0.225])\n",
    "trainloader = torch.utils.data.DataLoader(\n",
    "        datasets.CIFAR100(root='./data', train=True, transform=transforms.Compose([\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.RandomCrop(32, 4),\n",
    "            transforms.ToTensor(),\n",
    "            normalize,\n",
    "        ]), download=True),\n",
    "        batch_size=500, shuffle=True,\n",
    "        num_workers=4, pin_memory=True)\n",
    "\n",
    "testloader = torch.utils.data.DataLoader(\n",
    "        datasets.CIFAR100(root='./data', train=False, transform=transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            normalize,\n",
    "        ])),\n",
    "        batch_size=500, shuffle=False,\n",
    "        num_workers=4, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _weights_init(m):\n",
    "    \"\"\"\n",
    "        Initialization of CNN weights\n",
    "    \"\"\"\n",
    "    classname = m.__class__.__name__\n",
    "    if isinstance(m, nn.Linear) or isinstance(m, nn.Conv2d):\n",
    "        init.kaiming_normal_(m.weight)\n",
    "\n",
    "\n",
    "class LambdaLayer(nn.Module):\n",
    "    \"\"\"\n",
    "      Identity mapping between ResNet blocks with diffrenet size feature map\n",
    "    \"\"\"\n",
    "    def __init__(self, lambd):\n",
    "        super(LambdaLayer, self).__init__()\n",
    "        self.lambd = lambd\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.lambd(x)\n",
    "\n",
    "# A basic block as shown in Fig.3 (right) in the paper consists of two convolutional blocks, each followed by a Bach-Norm layer. \n",
    "# Every basic block is shortcuted in ResNet architecture to construct f(x)+x module. \n",
    "# Expansion for option 'A' in the paper is equal to identity with extra zero entries padded\n",
    "# for increasing dimensions between layers with different feature map size. This option introduces no extra parameter. \n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1, option='A'):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != planes:\n",
    "            if option == 'A':\n",
    "                \"\"\"\n",
    "                For CIFAR10 experiment, ResNet paper uses option A.\n",
    "                \"\"\"\n",
    "                self.shortcut = LambdaLayer(lambda x:\n",
    "                                            F.pad(x[:, :, ::2, ::2], (0, 0, 0, 0, planes//4, planes//4), \"constant\", 0))\n",
    "            elif option == 'B':\n",
    "                self.shortcut = nn.Sequential(\n",
    "                     nn.Conv2d(in_planes, self.expansion * planes, kernel_size=1, stride=stride, bias=False),\n",
    "                     nn.BatchNorm2d(self.expansion * planes)\n",
    "                )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "# Stack of 3 times 2*n (n is the number of basic blocks) layers are used for making the ResNet model, \n",
    "# where each 2n layers have feature maps of size {16,32,64}, respectively. \n",
    "# The subsampling is performed by convolutions with a stride of 2.\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=100):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = 16\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.layer1 = self._make_layer(block, 16, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 32, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 64, num_blocks[2], stride=2)\n",
    "        self.linear = nn.Linear(64, num_classes)\n",
    "        self.apply(_weights_init)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = F.avg_pool2d(out, out.size()[3])\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "def resnet20():\n",
    "    return ResNet(BasicBlock, [3, 3, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Training Routine\n",
    "def training_routine(model, train_generator, test_generator, n_epochs, writer = writer,  \n",
    "                     eval_every=5):\n",
    "    \n",
    "    model.to(DEVICE)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr =0.1, \n",
    "                                momentum = 0.9, weight_decay = 0.001)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor =0.5, patience = 1)\n",
    "    accuracies = []\n",
    "    \n",
    "    for i in range(n_epochs):\n",
    "        # Iterate over batches\n",
    "        batch_losses = []\n",
    "        \n",
    "        for X_batch, y_batch in train_generator:\n",
    "            optimizer.zero_grad()\n",
    "            # forward pass\n",
    "            X_batch, y_batch = X_batch.to(DEVICE), y_batch.to(DEVICE)\n",
    "            batch_output = model(X_batch)\n",
    "            batch_loss = criterion(batch_output, y_batch)\n",
    "            # backward pass and optimization\n",
    "            batch_loss.backward()\n",
    "            optimizer.step()\n",
    "            batch_losses.append(batch_loss.cpu().detach())\n",
    "        print(\"Epoch {} | training loss: {}\".format(i, np.mean(batch_losses)))\n",
    "        writer.add_scalar('Loss/train', np.mean(batch_losses), i)\n",
    "       \n",
    "        \n",
    "        # Once every 100 iterations, print statistics\n",
    "        if i%eval_every==0:\n",
    "            train_accuracy = []\n",
    "            test_accuracy = []\n",
    "            for X_batch, y_batch in train_generator:\n",
    "                X_batch, y_batch = X_batch.to(DEVICE), y_batch.to(DEVICE)\n",
    "                batch_output = model(X_batch)\n",
    "                batch_prediction = batch_output.cpu().detach().argmax(dim=1)\n",
    "                train_accuracy.append((batch_prediction.numpy()==y_batch.cpu().numpy()).mean())\n",
    "                \n",
    "            for X_batch, y_batch in test_generator:\n",
    "                X_batch, y_batch = X_batch.to(DEVICE), y_batch.to(DEVICE)\n",
    "                batch_output = model(X_batch)\n",
    "                batch_prediction = batch_output.cpu().detach().argmax(dim=1)\n",
    "                test_accuracy.append((batch_prediction.numpy()==y_batch.cpu().numpy()).mean())\n",
    "            print(\"Epoch {} | train acc: {}, test acc: {}\".format(i, np.mean(train_accuracy), np.mean(test_accuracy)))\n",
    "            writer.add_scalar('Accuracy/train', np.mean(train_accuracy), i)\n",
    "            writer.add_scalar('Accuracy/test', np.mean(test_accuracy), i)\n",
    "            scheduler.step(np.mean(test_accuracy))\n",
    "            accuracies.append((i, np.mean(train_accuracy), np.mean(test_accuracy)))\n",
    "            \n",
    "    return model.cpu(), accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 | training loss: 4.499466419219971\n",
      "Epoch 0 | train acc: 0.0666, test acc: 0.06330000000000001\n",
      "Epoch 1 | training loss: 3.979645013809204\n",
      "Epoch 2 | training loss: 3.6204090118408203\n",
      "Epoch 3 | training loss: 3.391629219055176\n",
      "Epoch 4 | training loss: 3.129716396331787\n",
      "Epoch 5 | training loss: 2.857428789138794\n",
      "Epoch 5 | train acc: 0.29943999999999993, test acc: 0.2884\n",
      "Epoch 6 | training loss: 2.674858331680298\n",
      "Epoch 7 | training loss: 2.5371713638305664\n",
      "Epoch 8 | training loss: 2.3982579708099365\n",
      "Epoch 9 | training loss: 2.2790615558624268\n",
      "Epoch 10 | training loss: 2.172884225845337\n",
      "Epoch 10 | train acc: 0.43467999999999996, test acc: 0.40630000000000005\n",
      "Epoch 11 | training loss: 1.9350450038909912\n",
      "Epoch 12 | training loss: 1.8679378032684326\n",
      "Epoch 13 | training loss: 1.821645975112915\n",
      "Epoch 14 | training loss: 1.7743632793426514\n",
      "Epoch 15 | training loss: 1.7472634315490723\n",
      "Epoch 15 | train acc: 0.5280199999999999, test acc: 0.48279999999999995\n",
      "Epoch 16 | training loss: 1.7212222814559937\n",
      "Epoch 17 | training loss: 1.6819267272949219\n",
      "Epoch 18 | training loss: 1.6462430953979492\n",
      "Epoch 19 | training loss: 1.6290141344070435\n",
      "Epoch 20 | training loss: 1.6039868593215942\n",
      "Epoch 20 | train acc: 0.5692400000000001, test acc: 0.5045999999999999\n",
      "Epoch 21 | training loss: 1.4440652132034302\n",
      "Epoch 22 | training loss: 1.4031882286071777\n",
      "Epoch 23 | training loss: 1.3837306499481201\n",
      "Epoch 24 | training loss: 1.3801649808883667\n",
      "Epoch 25 | training loss: 1.3590290546417236\n",
      "Epoch 25 | train acc: 0.62726, test acc: 0.5432\n",
      "Epoch 26 | training loss: 1.34995436668396\n",
      "Epoch 27 | training loss: 1.3447424173355103\n",
      "Epoch 28 | training loss: 1.3273167610168457\n",
      "Epoch 29 | training loss: 1.3186606168746948\n",
      "Epoch 30 | training loss: 1.314969778060913\n",
      "Epoch 30 | train acc: 0.63456, test acc: 0.5402000000000001\n",
      "Epoch 31 | training loss: 1.199507236480713\n",
      "Epoch 32 | training loss: 1.1678956747055054\n",
      "Epoch 33 | training loss: 1.1591274738311768\n",
      "Epoch 34 | training loss: 1.1511167287826538\n",
      "Epoch 35 | training loss: 1.143660068511963\n",
      "Epoch 35 | train acc: 0.6844600000000001, test acc: 0.5676000000000001\n",
      "Epoch 36 | training loss: 1.1341556310653687\n",
      "Epoch 37 | training loss: 1.1297224760055542\n",
      "Epoch 38 | training loss: 1.1183650493621826\n",
      "Epoch 39 | training loss: 1.1167850494384766\n",
      "Epoch 40 | training loss: 1.112545371055603\n",
      "Epoch 40 | train acc: 0.6874600000000001, test acc: 0.5723\n",
      "Epoch 41 | training loss: 1.0348052978515625\n",
      "Epoch 42 | training loss: 1.0159603357315063\n",
      "Epoch 43 | training loss: 1.0065644979476929\n",
      "Epoch 44 | training loss: 1.0014829635620117\n",
      "Epoch 45 | training loss: 1.0013940334320068\n",
      "Epoch 45 | train acc: 0.7187399999999999, test acc: 0.584\n",
      "Epoch 46 | training loss: 0.9907942414283752\n",
      "Epoch 47 | training loss: 0.9873278737068176\n",
      "Epoch 48 | training loss: 0.9794641137123108\n",
      "Epoch 49 | training loss: 0.9805065989494324\n",
      "Epoch 50 | training loss: 0.9771224856376648\n",
      "Epoch 50 | train acc: 0.72598, test acc: 0.5868\n",
      "11.760944596926372\n"
     ]
    }
   ],
   "source": [
    "model = resnet20()\n",
    "start = time.time()\n",
    "trained_net, accuracies = training_routine(model, trainloader, testloader, 51)\n",
    "end = time.time()\n",
    "print((end - start)/60) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dict = trained_net.state_dict()\n",
    "torch.save(model_dict, \"resnet20\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'count_params' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-b94aaa3c6be1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnum_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcount_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'count_params' is not defined"
     ]
    }
   ],
   "source": [
    "num_params = count_params(model)\n",
    "print(num_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = []\n",
    "model = model.cuda()\n",
    "model.eval()\n",
    "\n",
    "testloader = torch.utils.data.DataLoader(\n",
    "        datasets.CIFAR100(root='./data', train=False, transform=transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            normalize,\n",
    "        ])),\n",
    "        batch_size=100, shuffle=False,\n",
    "        num_workers=1, pin_memory=True)\n",
    "\n",
    "start = time.time()\n",
    "for local_batch, local_labels in testloader:\n",
    "    local_batch, local_labels = local_batch.cuda(), local_labels.cuda()\n",
    "    batch_output = model(local_batch)\n",
    "    batch_prediction = batch_output.cpu().detach().argmax(dim=1)\n",
    "    y_pred.append(batch_prediction)\n",
    "end = time.time()  \n",
    "\n",
    "y_pred = torch.cat(y_pred).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.021204571723937988"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(end-start)/100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, precision_score, recall_score, classification_report, confusion_matrix\n",
    "n_classes = 100\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.imshow(confusion_matrix(y_test, y_pred, labels=range(n_classes)))\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies = np.array(accuracies)\n",
    "fig, ax = plt.subplots(1, 1, figsize = (6, 4))\n",
    "ax.plot(accuracies[:, 0], accuracies[:, 1], label = \"Train\")\n",
    "ax.plot(accuracies[:, 0], accuracies[:, 2], label = 'Test')\n",
    "ax.legend(fontsize = 10)\n",
    "ax.set_xlim((0,50))\n",
    "ax.set_ylabel('Accuarcy', fontsize = 12)\n",
    "ax.set_xlabel(\"# of Epochs\", fontsize = 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
